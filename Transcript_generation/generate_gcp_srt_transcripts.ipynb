{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826ffc08-6574-4957-89f4-6d86ca79581c",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id=\"\" #GCP Project ID used to call the Speech-to-Text API (API must be enabled)\n",
    "gcs_audio_files_directory=\"\" #GCS URI to audio files in format gs://bucket_name/path, files must be stored in subdirectories named after language code of the video (i.e. en-US)\n",
    "gcs_transcripts_directory=\"\" #GCS URI to store generated transcripts in format gs://bucket_name/path\n",
    "model=\"chirp\" #GCP model to use for transcription (i.e. chirp, long)\n",
    "max_characters_per_chunk=40 #Maximum number of characters per SRT chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a2294b-8ce0-4ee2-9223-0540a974ae96",
   "metadata": {},
   "outputs": [],
   "source": [
    "!(pip install google-cloud-speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329e433a-af05-4e0f-9b0c-dbd9cbaa12a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.api_core.client_options import ClientOptions\n",
    "from google.cloud.speech_v2.types import cloud_speech\n",
    "from google.cloud import speech_v2\n",
    "\n",
    "async def transcribe_batch_multiple_files_v2(\n",
    "    project_id: str,\n",
    "    file_array: [],\n",
    "    gcs_output_path: str,\n",
    "    model: str,\n",
    "):\n",
    "    if model==\"chirp\":\n",
    "        client = speech_v2.SpeechAsyncClient(\n",
    "            client_options=ClientOptions(\n",
    "                api_endpoint=\"europe-west4-speech.googleapis.com\",\n",
    "            )\n",
    "        )\n",
    "        location=\"europe-west4\"\n",
    "    if model==\"long\":\n",
    "        client = speech_v2.SpeechAsyncClient()\n",
    "        location=\"global\"\n",
    "\n",
    "    default_config = cloud_speech.RecognitionConfig(\n",
    "        auto_decoding_config=cloud_speech.AutoDetectDecodingConfig(),\n",
    "        language_codes=[\"en-US\"],\n",
    "        model=model,\n",
    "        features=cloud_speech.RecognitionFeatures(\n",
    "            enable_automatic_punctuation=True,\n",
    "            enable_word_time_offsets=True,\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    files=[]\n",
    "    \n",
    "    for file in file_array:\n",
    "        config = cloud_speech.RecognitionConfig(\n",
    "            language_codes=[file[\"language_code\"]],\n",
    "        )\n",
    "        files.append(\n",
    "            cloud_speech.BatchRecognizeFileMetadata(\n",
    "                uri=file[\"gcs_uri\"],\n",
    "                config=config,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    request = speech_v2.BatchRecognizeRequest(\n",
    "        recognizer=f\"projects/{project_id}/locations/{location}/recognizers/_\",\n",
    "        config=default_config,\n",
    "        files=files,\n",
    "        recognition_output_config=cloud_speech.RecognitionOutputConfig(\n",
    "            gcs_output_config=cloud_speech.GcsOutputConfig(\n",
    "                uri=gcs_output_path+\"/\"+model+\"/srt/json\",\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "    operation = client.batch_recognize(request=request,timeout=530)\n",
    "    print(\"Waiting for operation to complete...\")\n",
    "    response = await (await operation).result(timeout=530)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c29ef4-f97f-40af-8468-8ea5c7402702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "language_directories=!(gsutil ls {gcs_audio_files_directory})\n",
    "file_array=[]\n",
    "\n",
    "for language_directory in language_directories:\n",
    "    \n",
    "    language_code=re.search(\"/([^/]+)/$\",language_directory).group(1)\n",
    "    files=!(gsutil ls {language_directory})\n",
    "    for uri in files:\n",
    "        \n",
    "        file_array.append({\"gcs_uri\": uri,\"language_code\": language_code})\n",
    "\n",
    "print(file_array)\n",
    "number_of_files=len(file_array)\n",
    "print(f\"Number of files: {number_of_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04916570-4d07-489e-abac-c6520c6a478f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import asyncio\n",
    "\n",
    "def split_array_into_chunks(array, chunk_size):\n",
    "    \"\"\"\n",
    "    Splits an array into chunks of the specified size.\n",
    "\n",
    "    Args:\n",
    "        array: The array to split.\n",
    "        chunk_size: The size of each chunk.\n",
    "\n",
    "    Returns:\n",
    "        A list of chunks.\n",
    "    \"\"\"\n",
    "\n",
    "    chunks = []\n",
    "    for i in range(0, len(array), chunk_size):\n",
    "        chunks.append(array[i : i + chunk_size])\n",
    "\n",
    "    return chunks\n",
    "\n",
    "arrays=split_array_into_chunks(file_array, 5)\n",
    "\n",
    "print(arrays)\n",
    "start_time = time.time()\n",
    "await asyncio.gather(*[transcribe_batch_multiple_files_v2(project_id, array, gcs_transcripts_directory, model) for array in arrays])\n",
    "\n",
    "duration = time.time() - start_time\n",
    "print(f\"Took {duration} to execute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a59b23d-1ca1-4a2a-9d1e-d22b1b077b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!(pip install srt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5360cffa-ea30-4aea-90d9-0e10c0782994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import srt\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "def break_sentences(subs, alternative, max_chars=40):\n",
    "    firstword = True\n",
    "    charcount = 0\n",
    "    idx = len(subs) + 1\n",
    "    content = \"\"\n",
    "\n",
    "    for w in alternative[\"words\"]:\n",
    "        if firstword:\n",
    "            # first word in sentence, record start time\n",
    "            start = datetime.timedelta(seconds=float(re.search(\"([0-9\\.]+)\",w[\"startOffset\"]).group(1)))\n",
    "\n",
    "        charcount += len(w[\"word\"])\n",
    "        content += \" \" + w[\"word\"].strip()\n",
    "\n",
    "        if (\".\" in w[\"word\"] or \"!\" in w[\"word\"] or \"?\" in w[\"word\"] or\n",
    "                charcount > max_chars or\n",
    "                (\",\" in w[\"word\"] and not firstword)):\n",
    "            # break sentence at: . ! ? or line length exceeded\n",
    "            # also break if , and not first word\n",
    "            end = datetime.timedelta(seconds=float(re.search(\"([0-9\\.]+)\",w[\"endOffset\"]).group(1)))\n",
    "            subs.append(srt.Subtitle(index=idx,\n",
    "                                     start=start,\n",
    "                                     end=end,\n",
    "                                     content=srt.make_legal_content(content)))\n",
    "            firstword = True\n",
    "            idx += 1\n",
    "            content = \"\"\n",
    "            charcount = 0\n",
    "        else:\n",
    "            firstword = False\n",
    "    return subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb0116e-88c6-44b7-9459-dc419a250f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from google.cloud import storage\n",
    "\n",
    "transcript_array=!(gsutil ls {gcs_transcripts_directory}/{model}/srt/json)\n",
    "\n",
    "for transcript in transcript_array:\n",
    "    print(f\"Fetching results from {transcript}...\")\n",
    "    output_bucket, output_object = re.match(\n",
    "        r\"gs://([^/]+)/(.*)\", transcript\n",
    "    ).group(1, 2)\n",
    "\n",
    "    # Instantiates a Cloud Storage client\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    # Fetch results from Cloud Storage\n",
    "    bucket = storage_client.bucket(output_bucket)\n",
    "    blob_json = bucket.blob(output_object)\n",
    "    data = json.loads(blob_json.download_as_string(client=None))\n",
    "    \n",
    "    subs = []\n",
    "    \n",
    "    if \"results\" in data:\n",
    "        for result in data[\"results\"]:\n",
    "            if \"alternatives\" in result:\n",
    "                # First alternative is the most probable result\n",
    "                subs = break_sentences(subs, result[\"alternatives\"][0], max_characters_per_chunk)\n",
    "        \n",
    "    srt_filename=re.search(f\"/json/(.*)_transcript.*\",output_object).group(1)\n",
    "    srt_path=re.search(\"(.*)/json\",output_object).group(1)\n",
    "    \n",
    "    blob_srt = bucket.blob(f\"{srt_path}/srt/{srt_filename}.srt\")\n",
    "    blob_srt.upload_from_string(srt.compose(subs))"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m111"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
